# Awesome Visual Spatial Reasoning

<div align="center">
  <a href="https://song2yu.github.io/">Songsong Yu</a><sup>*1,2</sup>,
  <a href="https://github.com/Uason-Chen">Yuxin Chen</a><sup>üåü*2</sup>,
  <a href="https://haodot.github.io/">Hao Ju</a><sup>*3</sup>,
  <a href="https://github.com/JiaLianjie">Lianjie Jia</a><sup>*4</sup>,
  <a href="https://github.com/Dustzx">Fuxi Zhang</a><sup>4</sup>,
  <a href="https://spyflying.github.io/">Shaofei Huang</a><sup>3</sup>,
  <br>
  <a href="https://github.com/YuhanWu27">Yuhan Wu</a><sup>4</sup>,
  <a href="https://github.com/Cuirundi">Rundi Cui</a><sup>4</sup>,
  <a href="https://github.com/RBinghao">Binghao Ran</a><sup>4</sup>,
  <a href="https://scholar.google.com/citations?user=3SAk3GQAAAAJ&hl=en">Zhang Zaibin</a><sup>4</sup>,
  <a href="https://www.zdzheng.xyz/" target="_blank" >Zhedong Zheng</a><sup>3</sup>,
  <a href="https://zhipengzhang.cn/">Zhipeng Zhang</a><sup>1</sup>,
  <br>
  <a href="https://scholar.google.com/citations?user=j1XFhSoAAAAJ&hl=en&oi=ao" >Yifan Wang</a><sup>4</sup>,
  <a href="https://github.com/StevenGrove">Lin Song</a><sup>2</sup>,
  <a href="https://scholar.google.com/citations?hl=en&user=EfTwkXMolscC&view_op=list_works&sortby=pubdate" >Lijun Wang</a><sup>4</sup>,
  <a href="https://yanwei-li.com/">Yanwei Li</a><sup>‚úâÔ∏è5</sup>,
  <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en&oi=ao" >Ying Shan</a><sup>2</sup>,
  <a href="https://scholar.google.com/citations?hl=en&user=D3nE0agAAAAJ" >Huchuan Lu</a><sup>4</sup>,
  <br><br>
  <sup>1</sup>SJTU,
  <sup>2</sup>ARC Lab, Tencent PCG,
  <sup>3</sup>UM,
  <sup>4</sup>DLUT,
  <sup>5</sup>CUHK
  * Equal Contributions
  üåü Project Lead
  ‚úâÔ∏è Corresponding Author

<p align="center">
ü§ó <a href="https://huggingface.co/datasets/Two-hot/SIBench"> Dataset </a>&nbsp&nbsp | &nbsp&nbspüåê <a href="https://sibench.github.io/Awesome-Visual-Spatial-Reasoning/"> Leaderboard</a>&nbsp&nbsp | &nbsp&nbspüìä <a href="https://github.com/SIBench/Awesome-Visual-Spatial-Reasoning"> Survey </a>&nbsp&nbsp | &nbsp&nbspüéØ <a href="https://github.com/song2yu/SIBench-VSR"> Code </a>&nbsp&nbsp | &nbsp&nbspüìÑ <a href="https://arxiv.org/abs/2509.18905">arXiv</a>
</p>

</div>
<br>



<p align="center">
  <img src="https://www.tencent.net.cn/wp-content/uploads/2023/02/logo-vertical-white-1.png" width="105">
  <img src="https://upload.wikimedia.org/wikipedia/zh/d/d5/SJTU_emblem.svg" width="80">
  <img src="https://upload.wikimedia.org/wikipedia/commons/6/69/University_of_Macau_logo.svg" width="80">
  <img src="https://upload.wikimedia.org/wikipedia/zh/7/76/Dalian_University_of_Technology_logo.svg" width="80">
  <img src="https://upload.wikimedia.org/wikipedia/zh/0/09/CUHK_Coat_of_Arms.png" width="80">  

</p>

# News and Updates

- [x] üìúüìúüìú25.9.23 - Preprint a survey article on visual spatial reasoning tasks.
- [x] üéØüéØüéØ25.9.23 - Release comprehensive evaluation results of mainstream models in visual spatial reasoning.
- [x] üôåüëèüëê25.9.15 - [Open-source evaluation data](https://huggingface.co/datasets/Two-hot/SIBench) for visual spatial reasoning tasks.
- [x] ü§©ü•≥ü§ó25.9.15 - [Open-source evaluation toolkit](https://github.com/song2yu/SIBench_eval).
- [x] ‚úçÔ∏èü¶æüíº25.6.28 - Collected the "Datasets" section.
- [x] üèÉüèÉ‚Äç‚ôÄÔ∏èüèÉ‚Äç‚ôÇÔ∏è25.6.16 - The "Awesome Visual Spatial Reasoning" project is now live!
- [x] üëèüïÆüíª25.6.12 - The project has conducted research and collected 100 relevant works.
- [x] üôã‚Äç‚ôÄÔ∏èüôã‚Äç‚ôÇÔ∏èüôã25.6.10 - We launches a review project on visual spatial reasoning.



# Open-source evaluation toolkit

![radar2.6](./assets/radar2.6_calibri.png)

<h6 align="center"> Evaluation of SOTA Models on 23 Visual Spatial Reasoning Tasks.</h6>

[**Code Usage**](https://github.com/song2yu/SIBench_eval):

```bash
- git clone https://github.com/song2yu/SIBench-VSR.git
- Refer to the README.md for more details
```

# Contributing

> We welcome contributions to this repository! If you would like to contribute, please follow these steps:
>
> - Fork the repository.
> - Create a new branch with your changes.
> - Submit a pull request with a clear description of your changes.
>
> You can also open an issue if you have anything to add or comment.
>
> Please feel free to contact us (SongsongYu203@163.com).



# Overview

The research community is increasingly focused on the **visual spatial reasoning** (VSR) abilities of Vision-Language Models (VLMs). Yet, the field **lacks** a **clear overview of its evolution** and a **standardized benchmark for evaluation**. Current assessment methods are disparate and **lack a common toolkit**. This project aims to fill that void. We are developing a unified, comprehensive, and diverse evaluation toolkit, along with an accompanying survey paper. We are actively seeking collaboration and discussion with fellow experts to advance this initiative. 

# Task Explanation

Visual spatial understanding is a key task at the intersection of computer vision and cognitive science. It aims to **enable intelligent agents** (such as robots and AI systems) to **parse spatial relationships** in the environment **through visual inputs** (images, videos, etc.), forming an abstract cognition of the physical world. In Embodied Intelligence, it serves as the foundation for agents to achieve the "perception-decision-action" loop‚Äîonly by understanding attributes like object positions, distances, sizes, and orientations in space can intelligent agents navigate environments, manipulate objects, or interact with humans.



# Timeline

![Visual Spatial Intelligence-A Survey](./assets/Visual%20Spatial%20Intelligence-A%20Survey-1749795674636-2.svg)

# Citation

If you find this project useful, please consider citing:
```
@article{sibench2025,
  title={How Far are VLMs from True Visual Spatial Intelligence? A Benchmark-Driven Perspective},
  author={Songsong Yu, Yuxin Chen, Hao Ju, Lianjie Jia, Fuxi Zhang, Shaofei Huang, Yuhan Wu, Rundi Cui, Binghao Ran, Zaibin Zhang, Zhedong Zheng, Zhipeng Zhang, Yifan Wang, Lin Song, Lijun Wang, Yanwei Li, Ying Shan, Huchuan Lu},
  journal={arXiv preprint arXiv:2509.18905},
  year={2025}
}
```


# Table of Contents

To facilitate the community's quick understanding of visual-spatial reasoning, we first categorized it by input modalities into **Single image**, **Monocular Video**, and **Multi-View Images**. We also surveyed other input modalities such as point clouds, as well as specific applications like embodied robotics. These are temporarily grouped under "**Others**," and we will conduct a more detailed sorting in the future.

- [Evaluation Results](#evaluation-results)
- [Papers](#Papers)
  - [Single Image](#Single-Image)
  - [Monocular Video](#Monocular-Video)
  - [Multi-View Images](#Multi-View-Images)
  - [Others](#Others)
- [Datasets](#Datasets)

# Papers



## Single Image
<table>
<colgroup>
  <col style="width: 25%">
  <col style="width: 10%">
  <col style="width: 8%">
  <col style="width: 10%">
  <col style="width: 10%">
  <col style="width: 7%">
  <col style="width: 30%">
</colgroup>
<thead>
<tr>
  <th>Title</th>
  <th>Venue</th>
  <th>Date</th>
  <th>Code</th>
  <th>Stars</th>
  <th>Benchmark</th>
  <th>Illustration</th>
</tr>
</thead>
<tbody>
  <tr>
    <td><a href="https://openreview.net/pdf?id=Ku4lylDpjq">R2D3:ImpartingSpatial Reasoning by Reconstructing 3D Scenes from 2D Images</a></td>
    <td>ARXIV</td>
    <td>--</td>
    <td>--</td>
    <td>--</td>
    <td>R2D3</td>
    <td><img src="excel_images/ckJRxnty.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2506.04308">RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robtics</a></td>
    <td>ARXIV</td>
    <td>25-06</td>
    <td><a href="https://zhoues.github.io/RoboRefer/">link</a></td>
    <td>--</td>
    <td>RefSpatial-Bench</td>
    <td><img src="excel_images/3oBi4xlV.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2506.00123">Visual Embodied Brain: Let Multimodal Large Language Models See, Think, and Control in Spaces</a></td>
    <td>ARXIV</td>
    <td>25-06</td>
    <td><a href="https://github.com/OpenGVLab/VeBrain">link</a></td>
    <td><img src="https://img.shields.io/github/stars/OpenGVLab/VeBrain.svg?style=social&label=Star" alt="Star count" /></td>
    <td>VeBrain-600k</td>
    <td><img src="excel_images/GQnL7TTI.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2506.01371?">SVQA-R1: Reinforcing Spatial Reasoning in MLLMs via View-Consistent Reward Optimization</a></td>
    <td>ARXIV</td>
    <td>25-06</td>
    <td>--</td>
    <td>--</td>
    <td>SVQA-R1</td>
    <td><img src="excel_images/SIIpyeq2.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2506.03135">OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models</a></td>
    <td>--</td>
    <td>25-06</td>
    <td><a href="https://github.com/qizekun/OmniSpatial">link</a></td>
    <td><img src="https://img.shields.io/github/stars/qizekun/OmniSpatial.svg?style=social&label=Star" alt="Star count" /></td>
    <td>OmniSpatial</td>
    <td><img src="excel_images/G-D7ojpi.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2505.19015">Can Multimodal Large Language Models Understand Spatial Relations</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td><a href="https://github.com/ziyan-xiaoyu/SpatialMQA">link</a></td>
    <td><img src="https://img.shields.io/github/stars/ziyan-xiaoyu/SpatialMQA.svg?style=social&label=Star" alt="Star count" /></td>
    <td>SpatialMQA</td>
    <td><img src="excel_images/nUi7FK26.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2505.12448">SSR: Enhancing Depth Perception in Vision-Language Models via Rationale-Guided Spatial Reasoning</a></td>
    <td>--</td>
    <td>25-05</td>
    <td>--</td>
    <td>--</td>
    <td>SSR-CoT</td>
    <td><img src="excel_images/bKnaWMGw.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2505.12194">Spatial-LLaVA: Enhancing Large Language Models with Spatial Referring Expressions for Visual Understanding</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td><a href="https://arpg.github.io/sunspot/">link</a></td>
    <td>--</td>
    <td>SUNSPOT</td>
    <td><img src="excel_images/piMdlSVH.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2505.11907">Are Multimodal Large Language Models Ready for Omnidirectional Spatial Reasoning?</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td><a href="https://huggingface.co/datasets/UUUserna/OSR-Bench">link</a></td>
    <td>--</td>
    <td>OSR-Bench</td>
    <td><img src="excel_images/CWtVitlq.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2505.05456">SITE: towards Spatial Intelligence Thorough Evaluation</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td><a href="https://github.com/wenqi-wang20/SITE-Bench">link</a></td>
    <td><img src="https://img.shields.io/github/stars/wenqi-wang20/SITE-Bench.svg?style=social&label=Star" alt="Star count" /></td>
    <td>SITE</td>
    <td><img src="excel_images/PjmCzksd.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2505.15966">Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td>--</td>
    <td>--</td>
    <td>TallyQA, V*<br>InfographicVQA, MVBench</td>
    <td><img src="excel_images/If5Ux9D7.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2504.00883">Improved Visual-Spatial Reasoning via R1-Zero-Like Training</a></td>
    <td>ARXIV</td>
    <td>25-04</td>
    <td><a href="https://github.com/zhijie-group/R1-Zero-VSI">link</a></td>
    <td><img src="https://img.shields.io/github/stars/zhijie-group/R1-Zero-VSI.svg?style=social&label=Star" alt="Star count" /></td>
    <td>VSI-100K</td>
    <td><img src="excel_images/XrlzDCJM.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2504.17207">Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery Simulation</a></td>
    <td>ARXIV</td>
    <td>25-04</td>
    <td><a href="https://github.com/KAIST-Visual-AI-Group/APC-VLM">link</a></td>
    <td><img src="https://img.shields.io/github/stars/KAIST-Visual-AI-Group/APC-VLM.svg?style=social&label=Star" alt="Star count" /></td>
    <td>COMFORT++, 3DSRBench</td>
    <td><img src="excel_images/ke1Os4qo.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2504.20024?">SpatialReasoner: Towards Explicit and Generalizable 3D Spatial Reasoning</a></td>
    <td>ARXIV</td>
    <td>25-04</td>
    <td><a href="https://spatial-reasoner.github.io/">link</a></td>
    <td>--</td>
    <td>--</td>
    <td><img src="excel_images/iLRbohFl.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2504.16061">Vision language models are unreliable at trivial spatial cognition</a></td>
    <td>ARXIV</td>
    <td>25-04</td>
    <td>--</td>
    <td>--</td>
    <td>TableTest</td>
    <td><img src="excel_images/q7qVO_ep.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2504.20648">SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data</a></td>
    <td>ARXIV</td>
    <td>25-04</td>
    <td>--</td>
    <td>--</td>
    <td>vsr, what's up<br>3DSR-Bench, RealWorldQA</td>
    <td><img src="excel_images/EPevRnTy.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2504.03164">NUSCENES-SPATIALQA: A Spatial Understanding and Reasoning Benchmark for Vision-Language Models in Autonomous Driving</a></td>
    <td>ARXIV</td>
    <td>25-04</td>
    <td><a href="https://github.com/taco-group/NuScenes-SpatialQA">link</a></td>
    <td><img src="https://img.shields.io/github/stars/taco-group/NuScenes-SpatialQA.svg?style=social&label=Star" alt="Star count" /></td>
    <td>NuScenes-SpatialQA</td>
    <td><img src="excel_images/5RGTd03H.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2503.17349">Beyond Semantics Rediscovering Spatial Awareness in Vision-Language Models</a></td>
    <td>--</td>
    <td>25-03</td>
    <td><a href="https://user074.github.io/respatialaware/">link</a></td>
    <td>--</td>
    <td>--</td>
    <td><img src="excel_images/KjKPB6al.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2503.18470">MetaSpatial: Reinforcing 3D Spatial Reasoning in
VLMs for the Metaverse</a></td>
    <td>ARXIV</td>
    <td>25-03</td>
    <td><a href="https://github.com/PzySeere/MetaSpatial">link</a></td>
    <td><img src="https://img.shields.io/github/stars/PzySeere/MetaSpatial.svg?style=social&label=Star" alt="Star count" /></td>
    <td>MetaSpatial</td>
    <td><img src="excel_images/7mXKL_Xd.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2503.19707?">Mind the Gap: Benchmarking Spatial Reasoning in Vision-Language Models</a></td>
    <td>ARXIV</td>
    <td>25-03</td>
    <td><a href="https://github.com/stogiannidis/srbench">link</a></td>
    <td><img src="https://img.shields.io/github/stars/stogiannidis/srbench.svg?style=social&label=Star" alt="Star count" /></td>
    <td>SRBench</td>
    <td><img src="excel_images/iMKiAPNx.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2503.11094">Open3DVQA: ABenchmark for Comprehensive Spatial Reasoning with Multimodal Large Language Model in Open Space</a></td>
    <td>ARXIV</td>
    <td>25-03</td>
    <td><a href="https://github.com/WeichenZh/Open3DVQA">link</a></td>
    <td><img src="https://img.shields.io/github/stars/WeichenZh/Open3DVQA.svg?style=social&label=Star" alt="Star count" /></td>
    <td>Open3DVQA</td>
    <td><img src="excel_images/B6rsQ70w.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2503.07557">AutoSpatial: Visual-Language Reasoning for Social Robot Navigation through Efficient Spatial Reasoning Learning</a></td>
    <td>ARXIV</td>
    <td>25-03</td>
    <td><a href="https://github.com/Yanko96/AutoSpatial">link</a></td>
    <td><img src="https://img.shields.io/github/stars/Yanko96/AutoSpatial.svg?style=social&label=Star" alt="Star count" /></td>
    <td>AutoSpatial</td>
    <td><img src="excel_images/UhviONFs.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2503.01773">Why Is Spatial Reasoning Hard for VLMs? AnAttention Mechanism Perspective on Focus Areas</a></td>
    <td>ARXIV</td>
    <td>25-03</td>
    <td><a href="https://github.com/shiqichen17/AdaptVis">link</a></td>
    <td><img src="https://img.shields.io/github/stars/shiqichen17/AdaptVis.svg?style=social&label=Star" alt="Star count" /></td>
    <td>--</td>
    <td><img src="excel_images/LwXsyZMC.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2503.19990">LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning</a></td>
    <td>ARXIV</td>
    <td>25-03</td>
    <td><a href="https://github.com/Tangkexian/LEGO-Puzzles">link</a></td>
    <td><img src="https://img.shields.io/github/stars/Tangkexian/LEGO-Puzzles.svg?style=social&label=Star" alt="Star count" /></td>
    <td>LEGO-Puzzles</td>
    <td><img src="excel_images/kVSIpEvv.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2502.06787">Visual Agentic AI for Spatial Reasoning with a Dynamic API</a></td>
    <td>ARXIV</td>
    <td>25-02</td>
    <td><a href="https://github.com/damianomarsili/VADAR">link</a></td>
    <td><img src="https://img.shields.io/github/stars/damianomarsili/VADAR.svg?style=social&label=Star" alt="Star count" /></td>
    <td>--</td>
    <td><img src="excel_images/oyBut2Id.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2502.03214">iVISPAR ‚ÄîAnInteractive Visual-Spatial Reasoning Benchmark for VLMs</a></td>
    <td>ARXIV</td>
    <td>25-02</td>
    <td><a href="https://github.com/SharkyBamboozle/iVISPAR">link</a></td>
    <td><img src="https://img.shields.io/github/stars/SharkyBamboozle/iVISPAR.svg?style=social&label=Star" alt="Star count" /></td>
    <td>iVISPAR</td>
    <td><img src="excel_images/dxnn-oSj.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2502.06787">Visual Agentic AI for Spatial Reasoning with a Dynamic API</a></td>
    <td>ARXIV</td>
    <td>25-02</td>
    <td><a href="https://github.com/damianomarsili/VADAR">link</a></td>
    <td><img src="https://img.shields.io/github/stars/damianomarsili/VADAR.svg?style=social&label=Star" alt="Star count" /></td>
    <td>Q-Spatial Bench, VSI-Bench</td>
    <td><img src="excel_images/qe3Yk61K.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2502.11859">Defining and Evaluating Visual Language Models‚Äô Basic Spatial Abilities: A Perspective from Psychometrics</a></td>
    <td>ARXIV</td>
    <td>25-02</td>
    <td>--</td>
    <td>--</td>
    <td>BSA-Tests</td>
    <td><img src="excel_images/9t1bESiT.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2502.08317">Mitigating Hallucinations in Multimodal Spatial Relations through Constraint-Aware Prompting</a></td>
    <td>NAACL</td>
    <td>25-02</td>
    <td><a href="https://github.com/TIGER-AI-Lab/Pixel-Reasoner">link</a></td>
    <td><img src="https://img.shields.io/github/stars/TIGER-AI-Lab/Pixel-Reasoner.svg?style=social&label=Star" alt="Star count" /></td>
    <td>ARO, GQA<br>MMRel</td>
    <td><img src="excel_images/_vTf7DcB.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://openreview.net/pdf?id=84pDoCD4lH">Do Vision-Language Models Represent Space and How. Evaluating Spatial Frame of Reference under Ambiguities</a></td>
    <td>ICLR</td>
    <td>25-01</td>
    <td><a href="https://spatial-comfort.github.io/">link</a></td>
    <td>--</td>
    <td>COMFORT</td>
    <td><img src="excel_images/CXSpWyMR.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models_CVPR_2025_paper.pdf">ROBOSPATIAL: Teaching Spatial Understanding to 2D and 3D Vision-Language
 Models for Robotics</a></td>
    <td>CVPR</td>
    <td>25-01</td>
    <td>--</td>
    <td>--</td>
    <td>--</td>
    <td><img src="excel_images/TC__2UgV.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large_CVPR_2025_paper.pdf">Spatial457: A Diagnostic Benchmark for 6D Spatial Reasoning of Large Multimodal Models</a></td>
    <td>CVPR</td>
    <td>25-01</td>
    <td>--</td>
    <td>--</td>
    <td>--</td>
    <td><img src="excel_images/BShseUpI.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_Coarse_Correspondences_Boost_Spatial-Temporal_Reasoning_in_Multimodal_Language_Model_CVPR_2025_paper.pdf">COARSE CORRESPONDENCES Boost Spatial-Temporal Reasoning
 in Multimodal Language Model</a></td>
    <td>CVPR</td>
    <td>25-01</td>
    <td><a href="https://coarse-correspondence.github.io/">link</a></td>
    <td>--</td>
    <td>--</td>
    <td><img src="excel_images/tC2Vduxd.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models_CVPR_2025_paper.pdf">SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent Large Multimodal Models</a></td>
    <td>CVPR</td>
    <td>25-01</td>
    <td><a href="https://3d-spatial-reasoning.github.io/spatial-llm/">link</a></td>
    <td>--</td>
    <td>--</td>
    <td><img src="excel_images/rpslJvlI.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_SpatialCLIP_Learning_3D-aware_Image_Representations_from_Spatially_Discriminative_Language_CVPR_2025_paper.pdf">SpatialCLIP: Learning 3D-aware Image Representations from Spatially Discriminative Language</a></td>
    <td>CVPR</td>
    <td>25-01</td>
    <td><a href="https://github.com/SpatialVision/Spatial-CLIP">link</a></td>
    <td><img src="https://img.shields.io/github/stars/SpatialVision/Spatial-CLIP.svg?style=social&label=Star" alt="Star count" /></td>
    <td>SpatialBench</td>
    <td><img src="excel_images/BlRkUzGP.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2501.10074">SpatialCoT: Advancing Spatial Reasoning through Coordinate Alignment and Chain-of-Thought for Embodied Task Planning</a></td>
    <td>ARXIV</td>
    <td>25-01</td>
    <td>--</td>
    <td>--</td>
    <td>--</td>
    <td><img src="excel_images/FmXW7m8E.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_ReasonGrounder_LVLM-Guided_Hierarchical_Feature_Splatting_for_Open-Vocabulary_3D_Visual_Grounding_CVPR_2025_paper.pdf">ReasonGrounder: LVLM-Guided Hierarchical Feature Splatting for Open-Vocabulary 3D Visual Grounding and Reasoning</a></td>
    <td>CVPR</td>
    <td>25-01</td>
    <td><a href="https://zhenyangliu.github.io/ReasonGrounder/">link</a></td>
    <td>--</td>
    <td>ReasoningGD</td>
    <td><img src="excel_images/9XnVbt3j.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2501.07542">Imagine while Reasoning in Space: Multimodal Visualization-of-Thought</a></td>
    <td>ARXIV</td>
    <td>25-01</td>
    <td>--</td>
    <td>--</td>
    <td>LEC23, WMS+24<br>LZZ+24, RDT+24</td>
    <td><img src="excel_images/XW5XQWXS.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2412.06322">LLaVA-SpaceSGG: Visual Instruct Tuning for Open-vocabulary Scene Graph Generation with Enhanced Spatial Relations</a></td>
    <td>ARXIV</td>
    <td>24-12</td>
    <td><a href="https://github.com/Endlinc/LLaVA-SpaceSGG">link</a></td>
    <td><img src="https://img.shields.io/github/stars/Endlinc/LLaVA-SpaceSGG.svg?style=social&label=Star" alt="Star count" /></td>
    <td>SpaceSGG</td>
    <td><img src="excel_images/0rzzrmw0.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2412.07825">3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark</a></td>
    <td>ARXIV</td>
    <td>24-12</td>
    <td><a href="https://3dsrbench.github.io/">link</a></td>
    <td>--</td>
    <td>3DSRBench</td>
    <td><img src="excel_images/fO6STizy.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2412.12693">SPHERE: Unveiling Spatial Blind Spots in Vision-Language Models Through Hierarchical Evaluation</a></td>
    <td>ACL</td>
    <td>24-12</td>
    <td><a href="https://github.com/zwenyu/SPHERE-VLM">link</a></td>
    <td><img src="https://img.shields.io/github/stars/zwenyu/SPHERE-VLM.svg?style=social&label=Star" alt="Star count" /></td>
    <td>SPHERE</td>
    <td><img src="excel_images/7cEgaJSN.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2411.16425?">TopV-Nav: Unlocking the Top-View Spatial Reasoning Potential of MLLM for Zero-shot Object Navigation</a></td>
    <td>ARXIV</td>
    <td>24-11</td>
    <td>--</td>
    <td>--</td>
    <td>--</td>
    <td><img src="excel_images/pWj8q5S2.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2411.06048">AnEmpirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models</a></td>
    <td>EMNLP</td>
    <td>24-11</td>
    <td><a href="https://github.com/FatemehShiri/Spatial-MM">link</a></td>
    <td><img src="https://img.shields.io/github/stars/FatemehShiri/Spatial-MM.svg?style=social&label=Star" alt="Star count" /></td>
    <td>Spatial-MM</td>
    <td><img src="excel_images/uodFxl9V.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2411.15714">ROOT: VLM-based System for Indoor Scene Understanding and Beyond</a></td>
    <td>ARXIV</td>
    <td>24-11</td>
    <td><a href="https://github.com/harrytea/ROOT">link</a></td>
    <td><img src="https://img.shields.io/github/stars/harrytea/ROOT.svg?style=social&label=Star" alt="Star count" /></td>
    <td>SceneVLM</td>
    <td><img src="excel_images/DkVd5o2x.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2411.06048">An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models</a></td>
    <td>EMNLP 2024</td>
    <td>24-11</td>
    <td><a href="https://github.com/FatemehShiri/Spatial-MM">link</a></td>
    <td><img src="https://img.shields.io/github/stars/FatemehShiri/Spatial-MM.svg?style=social&label=Star" alt="Star count" /></td>
    <td>Spatial-MM, GQA-spatial</td>
    <td><img src="excel_images/UtdtS5eg.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2411.19325">GEOBench-VLM: Benchmarking Vision-Language Models for Geospatial Tasks</a></td>
    <td>ARXIV</td>
    <td>24-11</td>
    <td><a href="https://github.com/The-AI-Alliance/GEO-Bench-VLM">link</a></td>
    <td><img src="https://img.shields.io/github/stars/The-AI-Alliance/GEO-Bench-VLM.svg?style=social&label=Star" alt="Star count" /></td>
    <td>GEOBench-VLM</td>
    <td><img src="excel_images/ysVTWqmx.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2410.16162">Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models Elicits Generalization to Spatial Reasoning</a></td>
    <td>NIPS</td>
    <td>24-10</td>
    <td>--</td>
    <td>--</td>
    <td>what's up, coco-spatial<br>GQA-spatial</td>
    <td><img src="excel_images/NiMgx_u5.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2409.09788">Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models</a></td>
    <td>ARXIV</td>
    <td>24-09</td>
    <td><a href="https://github.com/andrewliao11/Q-Spatial-Bench-code">link</a></td>
    <td><img src="https://img.shields.io/github/stars/andrewliao11/Q-Spatial-Bench-code.svg?style=social&label=Star" alt="Star count" /></td>
    <td>Q-Spatial Bench</td>
    <td><img src="excel_images/3jzwvWDX.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2409.17080?">Can Vision Language Models Learn from Visual Demonstrations of Ambiguous Spatial Reasoning?</a></td>
    <td>ARXIV</td>
    <td>24-09</td>
    <td><a href="https://github.com/groundlight/vlm-visual-demonstrations">link</a></td>
    <td><img src="https://img.shields.io/github/stars/groundlight/vlm-visual-demonstrations.svg?style=social&label=Star" alt="Star count" /></td>
    <td>SVAT</td>
    <td><img src="excel_images/BlKtFYvy.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2408.11748">Understanding Depth and Height Perception in Large Visual-Language Models</a></td>
    <td>CVPRW</td>
    <td>24-08</td>
    <td><a href="https://github.com/sacrcv/GeoMeter">link</a></td>
    <td><img src="https://img.shields.io/github/stars/sacrcv/GeoMeter.svg?style=social&label=Star" alt="Star count" /></td>
    <td>GeoMeter</td>
    <td><img src="excel_images/aiTEQ9pF.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2408.00754">Coarse Correspondences Boost Spatial-Temporal Reasoning in Multimodal Language Model</a></td>
    <td>--</td>
    <td>24-08</td>
    <td>--</td>
    <td>--</td>
    <td>ScanQA, OpenEQA‚Äôs episodic memory subset<br>EgoSchema, R2R<br>SQA3D</td>
    <td><img src="excel_images/Yk_gmPJt.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2407.01863">VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs</a></td>
    <td>ARXIV</td>
    <td>24-07</td>
    <td><a href="https://github.com/UCSB-NLP-Chang/Visual-Spatial-Planning">link</a></td>
    <td><img src="https://img.shields.io/github/stars/UCSB-NLP-Chang/Visual-Spatial-Planning.svg?style=social&label=Star" alt="Star count" /></td>
    <td>VSP</td>
    <td><img src="excel_images/WoF7cD1Y.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2406.13642">SpatialBot: Precise Spatial Understanding with Vision Language Models</a></td>
    <td>ICRA</td>
    <td>24-06</td>
    <td><a href="https://github.com/BAAI-DCAI/SpatialBot">link</a></td>
    <td><img src="https://img.shields.io/github/stars/BAAI-DCAI/SpatialBot.svg?style=social&label=Star" alt="Star count" /></td>
    <td>SpatialBench</td>
    <td><img src="excel_images/Yp-lmvJu.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2406.01584">SpatialRGPT: Grounded Spatial Reasoning in Vision-Language Models</a></td>
    <td>ARXIV</td>
    <td>24-06</td>
    <td><a href="https://github.com/AnjieCheng/SpatialRGPT">link</a></td>
    <td><img src="https://img.shields.io/github/stars/AnjieCheng/SpatialRGPT.svg?style=social&label=Star" alt="Star count" /></td>
    <td>SpatialRGPT-Bench</td>
    <td><img src="excel_images/mUxxMKyj.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2406.02537">TOPVIEWRS: Vision-Language Models as Top-View Spatial Reasoners</a></td>
    <td>ARXIV</td>
    <td>24-06</td>
    <td><a href="https://topviewrs.github.io/">link</a></td>
    <td>--</td>
    <td>--</td>
    <td><img src="excel_images/RRwEhOQ9.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2406.14852">Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models</a></td>
    <td>NIPS</td>
    <td>24-06</td>
    <td><a href="https://github.com/jiayuww/SpatialEval">link</a></td>
    <td><img src="https://img.shields.io/github/stars/jiayuww/SpatialEval.svg?style=social&label=Star" alt="Star count" /></td>
    <td>SpatialEval</td>
    <td><img src="excel_images/t1vNlkif.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2406.05756">EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models</a></td>
    <td>ARXIV</td>
    <td>24-06</td>
    <td><a href="https://github.com/mengfeidu/EmbSpatial-Bench">link</a></td>
    <td><img src="https://img.shields.io/github/stars/mengfeidu/EmbSpatial-Bench.svg?style=social&label=Star" alt="Star count" /></td>
    <td>EmbSpatial-Bench</td>
    <td><img src="excel_images/RQRpRxe3.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2406.13246">GSR-BENCH: A Benchmark for Grounded Spatial Reasoning Evaluation via Multimodal LLMs</a></td>
    <td>NEURIPS 2024 WORKSHOP</td>
    <td>24-06</td>
    <td>--</td>
    <td>--</td>
    <td>GSR-BENCH</td>
    <td><img src="excel_images/0kUl9A6D.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2406.10721">RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics</a></td>
    <td>CORL2024</td>
    <td>24-06</td>
    <td><a href="https://github.com/wentaoyuan/RoboPoint">link</a></td>
    <td><img src="https://img.shields.io/github/stars/wentaoyuan/RoboPoint.svg?style=social&label=Star" alt="Star count" /></td>
    <td>RoboPoint</td>
    <td><img src="excel_images/s2uNbyig.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2405.15064">Reframing Spatial Reasoning Evaluation in Language Models:A Real-World Simulation Benchmark for Qualitative Reasoning</a></td>
    <td>ARXIV</td>
    <td>24-05</td>
    <td>--</td>
    <td>--</td>
    <td>RoomSpace, bAbI<br>StepGame, SpartQA<br>SpaRTUN</td>
    <td><img src="excel_images/ZVZ3wWo2.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://dl.acm.org/doi/pdf/10.1145/3664647.3688990">RAG-Guided Large Language Models for Visual Spatial Description with Adaptive Hallucination Corrector</a></td>
    <td>ACMMM24</td>
    <td>24-05</td>
    <td>--</td>
    <td>--</td>
    <td>VSD</td>
    <td><img src="excel_images/NVXj7Nl_.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2404.03622">Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models</a></td>
    <td>NIPS</td>
    <td>24-04</td>
    <td><a href="https://github.com/microsoft/visualization-of-thought/">link</a></td>
    <td><img src="https://img.shields.io/github/stars/microsoft/visualization-of-thought.svg?style=social&label=Star" alt="Star count" /></td>
    <td>VoT</td>
    <td><img src="excel_images/zN5b3bxe.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2404.12390">BLINK: Multimodal Large Language Models Can See but Not Perceive</a></td>
    <td>ECCV</td>
    <td>24-04</td>
    <td><a href="https://github.com/zeyofu/BLINK_Benchmark">link</a></td>
    <td><img src="https://img.shields.io/github/stars/zeyofu/BLINK_Benchmark.svg?style=social&label=Star" alt="Star count" /></td>
    <td>BLINK</td>
    <td><img src="excel_images/SaQyKFgB.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2404.03658">Know Your Neighbors: Improving Single-View Reconstruction via Spatial Vision-Language Reasoning</a></td>
    <td>CVPR2024</td>
    <td>24-04</td>
    <td><a href="https://github.com/ruili3/Know-Your-Neighbors">link</a></td>
    <td><img src="https://img.shields.io/github/stars/ruili3/Know-Your-Neighbors.svg?style=social&label=Star" alt="Star count" /></td>
    <td>KITTI-360</td>
    <td><img src="excel_images/qY6F0NNr.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2403.16999">Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning</a></td>
    <td>--</td>
    <td>24-03</td>
    <td><a href="https://github.com/deepcs233/Visual-CoT">link</a></td>
    <td><img src="https://img.shields.io/github/stars/deepcs233/Visual-CoT.svg?style=social&label=Star" alt="Star count" /></td>
    <td>Visual-CoT</td>
    <td><img src="excel_images/hxTISDlB.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2403.00729">Can Transformers Capture Spatial Relations between Objects?</a></td>
    <td>ICLR</td>
    <td>24-03</td>
    <td><a href="https://github.com/AlvinWen428/spatial-relation-benchmark">link</a></td>
    <td><img src="https://img.shields.io/github/stars/AlvinWen428/spatial-relation-benchmark.svg?style=social&label=Star" alt="Star count" /></td>
    <td>SRP</td>
    <td><img src="excel_images/Y_Fd7Vcn.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2403.13438">SpatialPIN: Enhancing Spatial Reasoning Capabilities of Vision-Language Models through Prompting and Interacting 3D Priors</a></td>
    <td>NEURIPS2024</td>
    <td>24-03</td>
    <td><a href="https://github.com/dannymcy/zeroshot_task_hallucination_code?tab=readme-ov-file">link</a></td>
    <td><img src="https://img.shields.io/github/stars/dannymcy/zeroshot_task_hallucination_code.svg?style=social&label=Star" alt="Star count" /></td>
    <td>NOCS, RT-1<br>BridgeData V2, YCBInEOAT</td>
    <td><img src="excel_images/Vp7ncxim.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2401.12168">SpatialVLM Endowing Vision-Language Models with Spatial Reasoning Capabilities</a></td>
    <td>CVPR</td>
    <td>24-01</td>
    <td><a href="https://github.com/remyxai/VQASynth">link</a></td>
    <td><img src="https://img.shields.io/github/stars/remyxai/VQASynth.svg?style=social&label=Star" alt="Star count" /></td>
    <td>--</td>
    <td><img src="excel_images/7oi4xTrs.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://dl.acm.org/doi/pdf/10.1145/3664647.3688992">LLaVA-VSD: Large Language-and-Vision Assistant for Visual Spatial Description</a></td>
    <td>ACM MM</td>
    <td>24-01</td>
    <td>--</td>
    <td>--</td>
    <td>--</td>
    <td><img src="excel_images/Q8ZfyMgz.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2401.17862">Proximity QA: Unleashing the Power of Multi-Modal Large Language Models for Spatial Proximity Analysis</a></td>
    <td>ARXIV</td>
    <td>24-01</td>
    <td><a href="https://github.com/NorthSummer/ProximityQA">link</a></td>
    <td><img src="https://img.shields.io/github/stars/NorthSummer/ProximityQA.svg?style=social&label=Star" alt="Star count" /></td>
    <td>Proximity-110K</td>
    <td><img src="excel_images/4si-RdPS.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2311.05298">Improving Vision-and-Language Reasoning via Spatial Relations Modeling</a></td>
    <td>WACV</td>
    <td>23-11</td>
    <td>--</td>
    <td>--</td>
    <td>--</td>
    <td><img src="excel_images/NlwM82Z8.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2310.17914">3D-Aware Visual Question Answering about Parts, Poses and Occlusions</a></td>
    <td>NIPS</td>
    <td>23-10</td>
    <td><a href="https://github.com/XingruiWang/3D-Aware-VQA">link</a></td>
    <td><img src="https://img.shields.io/github/stars/XingruiWang/3D-Aware-VQA.svg?style=social&label=Star" alt="Star count" /></td>
    <td>Super-CLEVR-3D</td>
    <td><img src="excel_images/3wsPfq8a.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2203.08075">Things not Written in Text: Exploring Spatial Commonsense from Visual Signals</a></td>
    <td>ACL2022</td>
    <td>22-03</td>
    <td><a href="https://github.com/xxxiaol/spatial-commonsense">link</a></td>
    <td><img src="https://img.shields.io/github/stars/xxxiaol/spatial-commonsense.svg?style=social&label=Star" alt="Star count" /></td>
    <td>--</td>
    <td><img src="excel_images/mJC9VwMM.png" alt="img" /></td>
  </tr>
</tbody>
</table>


## Monocular-Video

<table>
<colgroup>
  <col style="width: 25%">
  <col style="width: 10%">
  <col style="width: 8%">
  <col style="width: 10%">
  <col style="width: 10%">
  <col style="width: 7%">
  <col style="width: 30%">
</colgroup>
<thead>
<tr>
  <th>Title</th>
  <th>Venue</th>
  <th>Date</th>
  <th>Code</th>
  <th>Stars</th>
  <th>Benchmark</th>
  <th>Illustration</th>
</tr>
</thead>
<tbody>
  <tr>
    <td><a href="https://vlm4d.github.io/file/VLM4D.pdf">VisualTrans: A Benchmark for Real-World Visual Transformation Reasoning</a></td>
    <td>ARXIV</td>
    <td>25-08</td>
    <td><a href="https://github.com/WangYipu2002/VisualTrans">link</a></td>
    <td>--</td>
    <td>VisualTrans</td>
    <td><img src="excel_images/144007655.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://vlm4d.github.io/file/VLM4D.pdf">VLM4D: Towards Spatiotemporal Awareness in Vision Language Models</a></td>
    <td>ICCV</td>
    <td>25-08</td>
    <td><a href="https://vlm4d.github.io/">link</a></td>
    <td>--</td>
    <td>VLM4D</td>
    <td><img src="excel_images/mvts2sIf.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Majumdar_OpenEQA_Embodied_Question_Answering_in_the_Era_of_Foundation_Models_CVPR_2024_paper.pdf">OpenEQA: Embodied Question Answering in the Era of Foundation Models</a></td>
    <td>CVPR</td>
    <td>--</td>
    <td><a href="https://github.com/facebookresearch/open-eqa">link</a></td>
    <td><img src="https://img.shields.io/github/stars/facebookresearch/open-eqa.svg?style=social&label=Star" alt="Star count" /></td>
    <td>OpenEQA</td>
    <td><img src="excel_images/ndzJ2BVb.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2506.03642">Spatial Understanding from Videos: Structured Prompts Meet Simulation Data</a></td>
    <td>ARXIV</td>
    <td>25-06</td>
    <td><a href="https://github.com/Hyu-Zhang/SpatialMind">link</a></td>
    <td><img src="https://img.shields.io/github/stars/Hyu-Zhang/SpatialMind.svg?style=social&label=Star" alt="Star count" /></td>
    <td>--</td>
    <td><img src="excel_images/IeDRr-38.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2505.20279">VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td><a href="https://github.com/VITA-Group/VLM-3R">link</a></td>
    <td><img src="https://img.shields.io/github/stars/VITA-Group/VLM-3R.svg?style=social&label=Star" alt="Star count" /></td>
    <td>VSTiBench</td>
    <td><img src="excel_images/uxYHjPjV.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2505.22657?">3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td><a href="https://github.com/3DLLM-Mem/3DLLM-Mem/">link</a></td>
    <td><img src="https://img.shields.io/github/stars/3DLLM-Mem/3DLLM-Mem.svg?style=social&label=Star" alt="Star count" /></td>
    <td>3DMEM-Bench</td>
    <td><img src="excel_images/Ewt2uUtb.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2505.23747">Spatial-MLLM Boosting MLLM Capabilities in Visual-based Spatial Intelligence</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td><a href="https://github.com/diankun-wu/Spatial-MLLM">link</a></td>
    <td><img src="https://img.shields.io/github/stars/diankun-wu/Spatial-MLLM.svg?style=social&label=Star" alt="Star count" /></td>
    <td>Spatial-MLLM-120k</td>
    <td><img src="excel_images/6YePvjP3.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2505.24257">Out of Sight, Not Out of Context? Egocentric Spatial Reasoning in VLMs Across Disjoint Frames</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td>--</td>
    <td>--</td>
    <td>DISJOINT-3DQA</td>
    <td><img src="excel_images/ELnMqB-1.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2505.24625">Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td><a href="https://github.com/LaVi-Lab/VG-LLM">link</a></td>
    <td><img src="https://img.shields.io/github/stars/LaVi-Lab/VG-LLM.svg?style=social&label=Star" alt="Star count" /></td>
    <td>VSI-Bench</td>
    <td><img src="excel_images/4mG8_HS-.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2505.04911?">SpatialPrompting: Keyframe-driven Zero-Shot Spatial Reasoning with Off-the-Shelf Multimodal Large Language Models</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td>--</td>
    <td>--</td>
    <td>--</td>
    <td><img src="excel_images/hckAkP5N.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2505.12363">Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td><a href="https://github.com/nkkbr/ViCA">link</a></td>
    <td><img src="https://img.shields.io/github/stars/nkkbr/ViCA.svg?style=social&label=Star" alt="Star count" /></td>
    <td>--</td>
    <td><img src="excel_images/4vSvUnxu.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2504.06958?">VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning</a></td>
    <td>ARXIV</td>
    <td>25-04</td>
    <td><a href="https://github.com/OpenGVLab/VideoChat-R1">link</a></td>
    <td><img src="https://img.shields.io/github/stars/OpenGVLab/VideoChat-R1.svg?style=social&label=Star" alt="Star count" /></td>
    <td>--</td>
    <td><img src="excel_images/ZjFI85dX.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2504.12680">Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning</a></td>
    <td>ARXIV</td>
    <td>25-04</td>
    <td><a href="https://github.com/EmbodiedCity/Embodied-R.code">link</a></td>
    <td><img src="https://img.shields.io/github/stars/EmbodiedCity/Embodied-R.code.svg?style=social&label=Star" alt="Star count" /></td>
    <td>Embodied-R</td>
    <td><img src="excel_images/Kx7d6rWf.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2504.01805v2">SpaceR: Reinforcing MLLMs in Video Spatial Reasoning</a></td>
    <td>ARXIV</td>
    <td>25-04</td>
    <td><a href="https://github.com/OuyangKun10/SpaceR">link</a></td>
    <td><img src="https://img.shields.io/github/stars/OuyangKun10/SpaceR.svg?style=social&label=Star" alt="Star count" /></td>
    <td>VSI-Bench, STI-Bench<br>and SPAR-Bench</td>
    <td><img src="excel_images/AWQIE78q.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2504.15376">Towards Understanding Camera Motions in Any Video</a></td>
    <td>ARXIV</td>
    <td>25-04</td>
    <td><a href="https://github.com/sy77777en/CameraBench">link</a></td>
    <td><img src="https://img.shields.io/github/stars/sy77777en/CameraBench.svg?style=social&label=Star" alt="Star count" /></td>
    <td>CameraBench</td>
    <td><img src="excel_images/aGVDnXId.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2503.15470">EgoDTM:Towards 3D-Aware Egocentric Video-Language Pretraining</a></td>
    <td>ARXIV</td>
    <td>25-03</td>
    <td><a href="https://github.com/xuboshen/EgoDTM">link</a></td>
    <td><img src="https://img.shields.io/github/stars/xuboshen/EgoDTM.svg?style=social&label=Star" alt="Star count" /></td>
    <td>EgoMCQ...</td>
    <td><img src="excel_images/-SVK1lFz.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2503.23765">STI-Bench: Are MLLMsReadyfor Precise Spatial-Temporal World Understanding?</a></td>
    <td>ARXIV</td>
    <td>25-03</td>
    <td><a href="https://github.com/MINT-SJTU/STI-Bench">link</a></td>
    <td><img src="https://img.shields.io/github/stars/MINT-SJTU/STI-Bench.svg?style=social&label=Star" alt="Star count" /></td>
    <td>STI-Bench</td>
    <td><img src="excel_images/VSDTmei6.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2503.12542?">ST-Think: How Multimodal Large Language Models Reason About 4D Worlds from Ego-Centric Videos</a></td>
    <td>ARXIV</td>
    <td>25-03</td>
    <td><a href="https://github.com/WPR001/Ego-ST">link</a></td>
    <td><img src="https://img.shields.io/github/stars/WPR001/Ego-ST.svg?style=social&label=Star" alt="Star count" /></td>
    <td>Ego-ST</td>
    <td><img src="excel_images/V0lxn_4p.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2503.22976?">From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D</a></td>
    <td>ARXIV</td>
    <td>25-03</td>
    <td><a href="https://github.com/fudan-zvg/spar">link</a></td>
    <td><img src="https://img.shields.io/github/stars/fudan-zvg/spar.svg?style=social&label=Star" alt="Star count" /></td>
    <td>SPAR-Bench</td>
    <td><img src="excel_images/yGaFgKPB.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2503.19355?">ST-VLM:KinematicInstruction Tuning for Spatio-Temporal Reasoning in Vision-Language Models</a></td>
    <td>ARXIV</td>
    <td>25-03</td>
    <td><a href="https://github.com/mlvlab/ST-VLM">link</a></td>
    <td><img src="https://img.shields.io/github/stars/mlvlab/ST-VLM.svg?style=social&label=Star" alt="Star count" /></td>
    <td>STKit</td>
    <td><img src="excel_images/Kja8ZtkC.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember_CVPR_2025_paper.pdf">Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces</a></td>
    <td>CVPR</td>
    <td>25-01</td>
    <td><a href="https://github.com/vision-x-nyu/thinking-in-space">link</a></td>
    <td><img src="https://img.shields.io/github/stars/vision-x-nyu/thinking-in-space.svg?style=social&label=Star" alt="Star count" /></td>
    <td>VSI-Bench</td>
    <td><img src="excel_images/n_ovbJvW.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://openreview.net/pdf?id=XYdstv3ySl">M3: 3D-Spatial Multimodal Memory</a></td>
    <td>ICLR</td>
    <td>25-01</td>
    <td><a href="https://github.com/MaureenZOU/m3-spatial">link</a></td>
    <td><img src="https://img.shields.io/github/stars/MaureenZOU/m3-spatial.svg?style=social&label=Star" alt="Star count" /></td>
    <td>--</td>
    <td><img src="excel_images/8-do6LRg.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Zheng_Video-3D_LLM_Learning_Position-Aware_Video_Representation_for_3D_Scene_Understanding_CVPR_2025_paper.pdf">Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding</a></td>
    <td>CVPR</td>
    <td>25-01</td>
    <td>--</td>
    <td>--</td>
    <td>--</td>
    <td><img src="excel_images/6x_7O5XO.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://openreview.net/pdf?id=6Vx28LSR7f">Compositional 4D Dynamic Scenes Understanding with Physics Priors for Video Question Answering</a></td>
    <td>ICLR</td>
    <td>25-01</td>
    <td><a href="https://github.com/XingruiWang/NS-4DPhysics">link</a></td>
    <td><img src="https://img.shields.io/github/stars/XingruiWang/NS-4DPhysics.svg?style=social&label=Star" alt="Star count" /></td>
    <td>DynSuperCLEVR</td>
    <td><img src="excel_images/hNS0e5hX.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2410.06468?">DOES SPATIAL COGNITION EMERGE IN FRONTIER MODELS?</a></td>
    <td>ICLR</td>
    <td>24-10</td>
    <td><a href="https://github.com/apple/ml-space-benchmark">link</a></td>
    <td><img src="https://img.shields.io/github/stars/apple/ml-space-benchmark.svg?style=social&label=Star" alt="Star count" /></td>
    <td>SPACE</td>
    <td><img src="excel_images/Zf8wEfmt.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2403.15941">Explore until Confident: Efficient Exploration for Embodied Question Answering</a></td>
    <td>ARXIV</td>
    <td>24-03</td>
    <td><a href="https://github.com/Stanford-ILIAD/explore-eqa">link</a></td>
    <td><img src="https://img.shields.io/github/stars/Stanford-ILIAD/explore-eqa.svg?style=social&label=Star" alt="Star count" /></td>
    <td>--</td>
    <td><img src="excel_images/yI4NZmPe.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_EmbodiedScan_A_Holistic_Multi-Modal_3D_Perception_Suite_Towards_Embodied_AI_CVPR_2024_paper.pdf">EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards Embodied AI</a></td>
    <td>CVPR</td>
    <td>24-01</td>
    <td>--</td>
    <td>--</td>
    <td>--</td>
    <td><img src="excel_images/daNXaoJU.png" alt="img" /></td>
  </tr>
</tbody>
</table>


## Multi-View Images

<table>
<colgroup>
  <col style="width: 25%">
  <col style="width: 10%">
  <col style="width: 8%">
  <col style="width: 10%">
  <col style="width: 10%">
  <col style="width: 7%">
  <col style="width: 30%">
</colgroup>
<thead>
<tr>
  <th>Title</th>
  <th>Venue</th>
  <th>Date</th>
  <th>Code</th>
  <th>Stars</th>
  <th>Benchmark</th>
  <th>Illustration</th>
</tr>
</thead>
<tbody>
  	<tr>
    <td><a href="https://arxiv.org/pdf/2506.18385">InternSpatial: A Comprehensive Dataset for Spatial Reasoning in Vision-Language Models</a></td>
    <td>ARXIV</td>
    <td>25-06</td>
    <td>--</td>
    <td>--</td>
    <td>InternSpatial</td>
    <td><img src="excel_images/_mz-0QYf.png" alt="img" /></td>
  </tr>
    <tr>
    <td><a href="https://arxiv.org/pdf/2505.17015">Multi-SpatialMLLM: Multi-Frame Spatial Understanding with MultiModal Large Language Models</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td><a href="https://github.com/facebookresearch/Multi-SpatialMLLM">link</a></td>
    <td><img src="https://img.shields.io/github/stars/facebookresearch/Multi-SpatialMLLM.svg?style=social&label=Star" alt="Star count" /></td>
    <td>--</td>
    <td><img src="excel_images/PMz_SQYf.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2505.23764?">MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td><a href="https://github.com/OpenRobotLab/MMSI-Bench">link</a></td>
    <td><img src="https://img.shields.io/github/stars/OpenRobotLab/MMSI-Bench.svg?style=social&label=Star" alt="Star count" /></td>
    <td>MMSI-Bench</td>
    <td><img src="excel_images/QruHRVLI.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2505.21500?">ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td><a href="https://github.com/ZJU-REAL/ViewSpatial-Bench">link</a></td>
    <td><img src="https://img.shields.io/github/stars/ZJU-REAL/ViewSpatial-Bench.svg?style=social&label=Star" alt="Star count" /></td>
    <td>ViewSpatial-Bench</td>
    <td><img src="excel_images/hkZ1mEvT.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2504.15280?">Seeing from Another Perspective Evaluating Multi-View Understanding in MLLMS</a></td>
    <td>ARXIV</td>
    <td>25-04</td>
    <td><a href="https://github.com/Chenyu-Wang567/All-Angles-Bench">link</a></td>
    <td><img src="https://img.shields.io/github/stars/Chenyu-Wang567/All-Angles-Bench.svg?style=social&label=Star" alt="Star count" /></td>
    <td>All-Angles-Bench</td>
    <td><img src="excel_images/Q4jeFZvX.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2503.13111">MM-Spatial Exploring 3D Spatial Understanding in Multimodal LLMs</a></td>
    <td>ARXIV</td>
    <td>25-03</td>
    <td>--</td>
    <td>--</td>
    <td>Cubify Anything VQA</td>
    <td><img src="excel_images/soG9FMUn.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhu_CoSpace_Benchmarking_Continuous_Space_Perception_Ability_for_Vision-Language_Models_CVPR_2025_paper.pdf">CoSpace: Benchmarking Continuous Space Perception Ability for Vision-Language Models</a></td>
    <td>CVPR</td>
    <td>25-01</td>
    <td>--</td>
    <td>--</td>
    <td>CoSpace</td>
    <td><img src="excel_images/lXwpriMO.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2412.07755">SAT: Dynamic Spatial Aptitude Training for Multimodal Language Models</a></td>
    <td>ARXIV</td>
    <td>24-12</td>
    <td>--</td>
    <td>--</td>
    <td>--</td>
    <td><img src="excel_images/wEgORe-h.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2412.04455">Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection</a></td>
    <td>CVPR2025</td>
    <td>24-12</td>
    <td>--</td>
    <td>--</td>
    <td>CLIPort, Omnigibson<br>RL-Bench</td>
    <td><img src="excel_images/sCm3ZUjW.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2003.14034">SPARE3D: A Dataset for SPAtial REasoning on Three-View Line Drawings</a></td>
    <td>CVPR2020</td>
    <td>20-03</td>
    <td><a href="https://github.com/ai4ce/SPARE3D">link</a></td>
    <td><img src="https://img.shields.io/github/stars/ai4ce/SPARE3D.svg?style=social&label=Star" alt="Star count" /></td>
    <td>SPARE3D</td>
    <td><img src="excel_images/49S30c8v.png" alt="img" /></td>
  </tr>
</tbody>
</table>





## Others

<table>
<colgroup>
  <col style="width: 25%">
  <col style="width: 10%">
  <col style="width: 8%">
  <col style="width: 10%">
  <col style="width: 10%">
  <col style="width: 7%">
  <col style="width: 30%">
</colgroup>
<thead>
<tr>
  <th>Title</th>
  <th>Venue</th>
  <th>Date</th>
  <th>Code</th>
  <th>Stars</th>
  <th>Benchmark</th>
  <th>Illustration</th>
</tr>
</thead>
<tbody>
  <tr>
    <td><a href="https://arxiv.org/pdf/2510.25760">Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks</a></td>
    <td>ARXIV</td>
    <td>25-10</td>
    <td><a href="https://github.com/zhengxuJosh/Awesome-Multimodal-Spatial-Reasoning">link</a></td>
    <td><img src="https://img.shields.io/github/stars/zhengxuJosh/Awesome-Multimodal-Spatial-Reasoning.svg?style=social&label=Star" alt="Star count" /></td>
    <td>--</td>
    <td><img src="excel_images/20251031164914_7_79.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2506.07966v1">SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence</a></td>
    <td>ARXIV</td>
    <td>25-06</td>
    <td><a href="https://github.com/VisionXLab/SpaCE-10">link</a></td>
    <td><img src="https://img.shields.io/github/stars/VisionXLab/SpaCE-10.svg?style=social&label=Star" alt="Star count" /></td>
    <td>SpaCE-10</td>
    <td><img src="excel_images/IpLLakdG.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2506.00070">Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics</a></td>
    <td>ARXIV</td>
    <td>25-06</td>
    <td>--</td>
    <td>--</td>
    <td>Robot-R1 Bench</td>
    <td><img src="excel_images/Ap5ZJDlU.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2506.04220">Struct2D: A Perception-Guided Framework for Spatial Reasoning in Large Multimodal Models</a></td>
    <td>ARXIV</td>
    <td>25-06</td>
    <td><a href="https://github.com/neu-vi/struct2d">link</a></td>
    <td><img src="https://img.shields.io/github/stars/neu-vi/struct2d.svg?style=social&label=Star" alt="Star count" /></td>
    <td>--</td>
    <td><img src="excel_images/Kj6G22Oj.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2505.17012">SpatialScore Towards Unified Evaluation for Multimodal Spatial Understanding</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td><a href="https://github.com/haoningwu3639/SpatialScore/">link</a></td>
    <td><img src="https://img.shields.io/github/stars/haoningwu3639/SpatialScore.svg?style=social&label=Star" alt="Star count" /></td>
    <td>SpatialScore</td>
    <td><img src="excel_images/OrXmF7wk.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2505.10875">A Light and Smart Wearable Platform with Multimodal Foundation Model for Enhanced Spatial Reasoning in People with Blindness and Low Vision</a></td>
    <td>ECCV</td>
    <td>25-05</td>
    <td>--</td>
    <td>--</td>
    <td>LVSQA</td>
    <td><img src="excel_images/tV74hVBA.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2505.09698">ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td><a href="https://manipbench.github.io/">link</a></td>
    <td>--</td>
    <td>ManipBench</td>
    <td><img src="excel_images/i_rCPxUY.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2505.13888">InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td><a href="https://github.com/Koorye/Inspire">link</a></td>
    <td><img src="https://img.shields.io/github/stars/Koorye/Inspire.svg?style=social&label=Star" alt="Star count" /></td>
    <td>--</td>
    <td><img src="excel_images/L3Hq2w00.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2505.22566">Universal Visuo-Tactile Video Understanding for Embodied Interaction</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td>--</td>
    <td>--</td>
    <td>--</td>
    <td><img src="excel_images/olwfm02V.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2505.20148">MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td><a href="https://github.com/MineAnyBuild/MineAnyBuild">link</a></td>
    <td><img src="https://img.shields.io/github/stars/MineAnyBuild/MineAnyBuild.svg?style=social&label=Star" alt="Star count" /></td>
    <td>--</td>
    <td><img src="excel_images/Cz-E9z1J.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2505.12253">LLaVA-4D: Embedding SpatioTemporal Prompt into LMMs for 4D Scene Understanding</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td>--</td>
    <td>--</td>
    <td>scan2cap, scanqa<br>scanref, multi3drefer<br>chat4d</td>
    <td><img src="excel_images/xt5mbBKy.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2504.15037">Scaling and Beyond: Advancing Spatial Reasoning in MLLMs Requires New Recipes</a></td>
    <td>ARXIV</td>
    <td>25-04</td>
    <td>--</td>
    <td>--</td>
    <td>--</td>
    <td><img src="excel_images/FwoHIlxY.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2504.09848">A Survey of Large Language Model-Powered Spatial Intelligence Across Scales: Advances in Embodied Agents, Smart Cities, and Earth Science</a></td>
    <td>ARXIV</td>
    <td>25-04</td>
    <td>--</td>
    <td>--</td>
    <td>--</td>
    <td><img src="excel_images/13nhoUDm.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2504.01901">Ross3D: Reconstructive Visual Instruction Tuning with 3D-Awareness</a></td>
    <td>ARXIV</td>
    <td>25-04</td>
    <td><a href="https://github.com/haochen-wang409/ross3d">link</a></td>
    <td><img src="https://img.shields.io/github/stars/haochen-wang409/ross3d.svg?style=social&label=Star" alt="Star count" /></td>
    <td>sqa3d, scanqa</td>
    <td><img src="excel_images/G0-EQ0TC.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2504.14151">Locate 3D: Real-World Object Localization via Self-Supervised Learning in 3D</a></td>
    <td>ARXIV</td>
    <td>25-04</td>
    <td><a href="https://github.com/facebookresearch/locate-3d">link</a></td>
    <td><img src="https://img.shields.io/github/stars/facebookresearch/locate-3d.svg?style=social&label=Star" alt="Star count" /></td>
    <td>SR3D, NR3D<br>ScanRefer</td>
    <td><img src="excel_images/n8DabBqd.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2503.13185">3DAxisPrompt: Promoting the 3D Grounding and Reasoning in GPT-4o</a></td>
    <td>--</td>
    <td>25-03</td>
    <td>--</td>
    <td>--</td>
    <td>--</td>
    <td><img src="excel_images/pS2z8p1E.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2503.11089">EmbodiedVSR: Dynamic Scene Graph-Guided Chain-of-Thought Reasoning for Visual Spatial Tasks</a></td>
    <td>ARXIV</td>
    <td>25-03</td>
    <td>--</td>
    <td>--</td>
    <td>--</td>
    <td><img src="excel_images/nG39O8iX.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2502.17775">FoREST: Frame of Reference Evaluation in Spatial Reasoning Tasks</a></td>
    <td>ARXIV</td>
    <td>25-02</td>
    <td><a href="https://github.com/HLR/FoREST">link</a></td>
    <td><img src="https://img.shields.io/github/stars/HLR/FoREST.svg?style=social&label=Star" alt="Star count" /></td>
    <td>FoREST</td>
    <td><img src="excel_images/N529Uhl9.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2502.08643">A Real-to-Sim-to-Real Approach to Robotic Manipulation with VLM-Generated Iterative Keypoint Rewards</a></td>
    <td>ICRA</td>
    <td>25-02</td>
    <td><a href="https://github.com/shivanshpatel35/IKER">link</a></td>
    <td><img src="https://img.shields.io/github/stars/shivanshpatel35/IKER.svg?style=social&label=Star" alt="Star count" /></td>
    <td>--</td>
    <td><img src="excel_images/TgT1tUOn.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2502.07183">pace-Aware Instruction Tuning: Dataset and Benchmark for Guide Dog Robots Assisting the Visually Impaired</a></td>
    <td>ARXIV</td>
    <td>25-02</td>
    <td><a href="https://github.com/byungokhan/Space-awareVLM">link</a></td>
    <td><img src="https://img.shields.io/github/stars/byungokhan/Space-awareVLM.svg?style=social&label=Star" alt="Star count" /></td>
    <td>SA-Bench</td>
    <td><img src="excel_images/AQaYM-9h.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2502.00931">VL-Nav: Real-time Vision-Language Navigation with Spatial Reasoning</a></td>
    <td>ARXIV</td>
    <td>25-02</td>
    <td>--</td>
    <td>--</td>
    <td>--</td>
    <td><img src="excel_images/-2pgOmfr.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2502.13143">SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation</a></td>
    <td>ARXIV</td>
    <td>25-02</td>
    <td><a href="https://github.com/qizekun/SoFar">link</a></td>
    <td><img src="https://img.shields.io/github/stars/qizekun/SoFar.svg?style=social&label=Star" alt="Star count" /></td>
    <td>--</td>
    <td><img src="excel_images/BjwAs8eo.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2501.16411">PHYSBENCH: BENCHMARKING AND ENHANCING VISION-LANGUAGE MODELS FOR PHYSICAL WORLD UNDERSTANDING</a></td>
    <td>ARXIV</td>
    <td>25-01</td>
    <td><a href="https://github.com/USC-GVL/PhysBench">link</a></td>
    <td><img src="https://img.shields.io/github/stars/USC-GVL/PhysBench.svg?style=social&label=Star" alt="Star count" /></td>
    <td>PhysBench</td>
    <td><img src="excel_images/Py0rydra.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_3D-Mem_3D_Scene_Memory_for_Embodied_Exploration_and_Reasoning_CVPR_2025_paper.pdf">3D-Mem: 3DScene Memory for Embodied Exploration and Reasoning</a></td>
    <td>CVPR</td>
    <td>25-01</td>
    <td><a href="https://github.com/UMass-Embodied-AGI/3D-Mem">link</a></td>
    <td><img src="https://img.shields.io/github/stars/UMass-Embodied-AGI/3D-Mem.svg?style=social&label=Star" alt="Star count" /></td>
    <td>--</td>
    <td><img src="excel_images/Lnskk-6M.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhou_PhysVLM_Enabling_Visual_Language_Models_to_Understand_Robotic_Physical_Reachability_CVPR_2025_paper.pdf">PhysVLM: Enabling Visual Language Models to Understand Robotic Physical Reachability</a></td>
    <td>CVPR</td>
    <td>25-01</td>
    <td>--</td>
    <td>--</td>
    <td>--</td>
    <td><img src="excel_images/V5aKqbhZ.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://www.tandfonline.com/doi/pdf/10.1080/13658816.2025.2490701">Evaluating and enhancing spatial cognition abilities of large language models</a></td>
    <td>IJGIS</td>
    <td>25-01</td>
    <td><a href="https://github.com/tbressers/WizardLM-2-8x22B">link</a></td>
    <td><img src="https://img.shields.io/github/stars/tbressers/WizardLM-2-8x22B.svg?style=social&label=Star" alt="Star count" /></td>
    <td>--</td>
    <td><img src="excel_images/mlZU-OuS.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2501.11858">EMBODIEDEVAL: Evaluate Multimodal LLMs as Embodied Agents</a></td>
    <td>ARXIV</td>
    <td>25-01</td>
    <td>--</td>
    <td>--</td>
    <td>EMBODIEDEVAL</td>
    <td><img src="excel_images/kich0R3E.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2501.09024">Social-LLaVA: Enhancing Robot Navigation through Human-Language Reasoning in Social Spaces</a></td>
    <td>ARXIV</td>
    <td>25-01</td>
    <td><a href="https://cs.gmu.edu/~xiao/Research/SNEI/">link</a></td>
    <td>--</td>
    <td>Social-LLaVA</td>
    <td><img src="excel_images/6TWRh14Z.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2501.15830">SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model</a></td>
    <td>ARXIV</td>
    <td>25-01</td>
    <td><a href="https://github.com/SpatialVLA/SpatialVLA">link</a></td>
    <td><img src="https://img.shields.io/github/stars/SpatialVLA/SpatialVLA.svg?style=social&label=Star" alt="Star count" /></td>
    <td>SpatialVLA</td>
    <td><img src="excel_images/YYL7PAD5.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2501.03841">OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints</a></td>
    <td>CVPR2025</td>
    <td>25-01</td>
    <td><a href="https://github.com/pmj110119/OmniManip">link</a></td>
    <td><img src="https://img.shields.io/github/stars/pmj110119/OmniManip.svg?style=social&label=Star" alt="Star count" /></td>
    <td>OmniManip</td>
    <td><img src="excel_images/RyTNWBnS.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2412.08619">Synthetic Vision: Training Vision-Language Models to Understand Physics</a></td>
    <td>--</td>
    <td>24-12</td>
    <td>--</td>
    <td>--</td>
    <td>--</td>
    <td><img src="excel_images/IwpfzC70.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2412.11974">Emma-X: An Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning</a></td>
    <td>--</td>
    <td>24-12</td>
    <td><a href="https://github.com/declare-lab/Emma-X">link</a></td>
    <td><img src="https://img.shields.io/github/stars/declare-lab/Emma-X.svg?style=social&label=Star" alt="Star count" /></td>
    <td>Emma-X</td>
    <td><img src="excel_images/hJo-wMvO.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2412.16599?">Do Multimodal Language Models Really Understand Direction? A Benchmark for Compass Direction Reasoning</a></td>
    <td>ARXIV</td>
    <td>24-12</td>
    <td>--</td>
    <td>--</td>
    <td>--</td>
    <td><img src="excel_images/hI77zphw.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10711245">GPT-4V(ision) for Robotics: Multimodal Task Planning From Human Demonstration</a></td>
    <td>OBOTICS AND AUTOMATION LETTERS</td>
    <td>24-11</td>
    <td>--</td>
    <td>--</td>
    <td>--</td>
    <td><img src="excel_images/A11Y4Oms.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2410.17385">Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference Under Ambiguities</a></td>
    <td>ICLR</td>
    <td>24-10</td>
    <td><a href="https://spatial-comfort.github.io/">link</a></td>
    <td>--</td>
    <td>COMFORT</td>
    <td><img src="excel_images/JSWnhPLE.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2407.14133">I KnowAbout‚ÄúUp‚Äù! Enhancing Spatial Reasoning in Visual Language Models Through 3D Reconstruction</a></td>
    <td>ARXIV</td>
    <td>24-07</td>
    <td>--</td>
    <td>--</td>
    <td>--</td>
    <td><img src="excel_images/8ZnNWOOF.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2407.01892?">GRASP: A Grid-Based Benchmark for Evaluating Commonsense Spatial Reasoning</a></td>
    <td>ARXIV</td>
    <td>24-07</td>
    <td><a href="https://github.com/jasontangzs0/GRASP">link</a></td>
    <td><img src="https://img.shields.io/github/stars/jasontangzs0/GRASP.svg?style=social&label=Star" alt="Star count" /></td>
    <td>--</td>
    <td><img src="excel_images/4Zeb5tPU.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2406.10721">RoboPoint: AVision-Language Model for Spatial Affordance Prediction for Robotics</a></td>
    <td>ARXIV</td>
    <td>24-06</td>
    <td><a href="https://github.com/wentaoyuan/RoboPoint">link</a></td>
    <td><img src="https://img.shields.io/github/stars/wentaoyuan/RoboPoint.svg?style=social&label=Star" alt="Star count" /></td>
    <td>Robopoint</td>
    <td><img src="excel_images/fsjntdiK.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2406.04566">SpaRC and SpaRP: Spatial Reasoning Characterization and Path Generation for Understanding Spatial Reasoning Capability of Large Language Models</a></td>
    <td>ACL</td>
    <td>24-06</td>
    <td><a href="https://github.com/UKPLab/acl2024-sparc-and-sparp">link</a></td>
    <td><img src="https://img.shields.io/github/stars/UKPLab/acl2024-sparc-and-sparp.svg?style=social&label=Star" alt="Star count" /></td>
    <td>SpaRP</td>
    <td><img src="excel_images/UEhJ-qFU.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2404.03658">KnowYour Neighbors: Improving Single-View Reconstruction via Spatial Vision-Language Reasoning</a></td>
    <td>CVPR</td>
    <td>24-04</td>
    <td><a href="https://github.com/ruili3/Know-Your-Neighbors">link</a></td>
    <td><img src="https://img.shields.io/github/stars/ruili3/Know-Your-Neighbors.svg?style=social&label=Star" alt="Star count" /></td>
    <td>--</td>
    <td><img src="excel_images/eiBYsRdx.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2403.11835">Agent3D-Zero: An Agent for Zero-shot 3D Understanding</a></td>
    <td>ECCV</td>
    <td>24-03</td>
    <td>--</td>
    <td>--</td>
    <td>--</td>
    <td><img src="excel_images/YyVnbk1b.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2403.11401">Scene-LLM: Extending Language Model for 3D Visual Understanding and Reasoning</a></td>
    <td>WACV</td>
    <td>24-03</td>
    <td>--</td>
    <td>--</td>
    <td>ScanQA, SQA3D<br>ALFRED</td>
    <td><img src="excel_images/CjCRBMoF.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2402.17766">ShapeLLM: Universal 3D Object Understanding for Embodied Interaction</a></td>
    <td>ECCV</td>
    <td>24-02</td>
    <td><a href="https://github.com/qizekun/ShapeLLM">link</a></td>
    <td><img src="https://img.shields.io/github/stars/qizekun/ShapeLLM.svg?style=social&label=Star" alt="Star count" /></td>
    <td>--</td>
    <td><img src="excel_images/PN4h5PzK.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2402.01591">BAT: Learning to Reason about Spatial Sounds with Large Language Models</a></td>
    <td>ARXIV</td>
    <td>24-02</td>
    <td><a href="https://github.com/zszheng147/Spatial-AST">link</a></td>
    <td><img src="https://img.shields.io/github/stars/zszheng147/Spatial-AST.svg?style=social&label=Star" alt="Star count" /></td>
    <td>--</td>
    <td><img src="excel_images/4HCfcLPj.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2402.03877">Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models</a></td>
    <td>ARXIV</td>
    <td>24-02</td>
    <td>--</td>
    <td>--</td>
    <td>Euclidea</td>
    <td><img src="excel_images/bnb8PV7y.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_LL3DA_Visual_Interactive_Instruction_Tuning_for_Omni-3D_Understanding_Reasoning_and_CVPR_2024_paper.pdf">LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning</a></td>
    <td>CVPR</td>
    <td>24-01</td>
    <td><a href="https://github.com/Open3DA/LL3DA">link</a></td>
    <td><img src="https://img.shields.io/github/stars/Open3DA/LL3DA.svg?style=social&label=Star" alt="Star count" /></td>
    <td>--</td>
    <td><img src="excel_images/7lPznAIW.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2401.03991">Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark</a></td>
    <td>AAAI</td>
    <td>24-01</td>
    <td>--</td>
    <td>--</td>
    <td>StepGame</td>
    <td><img src="excel_images/uMbGWH7c.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2401.00988">Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models</a></td>
    <td>CVPR</td>
    <td>24-01</td>
    <td><a href="https://github.com/xmed-lab/NuInstruct">link</a></td>
    <td><img src="https://img.shields.io/github/stars/xmed-lab/NuInstruct.svg?style=social&label=Star" alt="Star count" /></td>
    <td>NuInstruct</td>
    <td><img src="excel_images/zRv8XuZA.png" alt="img" /></td>
  </tr>
</tbody>
</table>

# Datasets

<table>
<colgroup>
  <col style="width: 25%">
  <col style="width: 10%">
  <col style="width: 8%">
  <col style="width: 15%">
  <col style="width: 8%">
  <col style="width: 10%">
  <col style="width: 24%">
</colgroup>
<thead>
<tr>
  <th>Title</th>
  <th>Venue</th>
  <th>Date</th>
  <th>Download-Link</th>
  <th>Citation</th>
  <th>Input Type</th>
  <th>Illustration</th>
</tr>
</thead>
<tbody>
  <tr>
    <td><a href="https://openreview.net/pdf?id=84pDoCD4lH">Do Vision-Language Models Represent Space and How. Evaluating Spatial Frame of Reference under Ambiguities</a></td>
    <td>ICLR</td>
    <td>25-11</td>
    <td><a href="https://huggingface.co/datasets/sled-umich/COMFORT/tree/main">link</a></td>
    <td>0</td>
    <td>Image</td>
    <td><img src="excel_images/dMM6N0W5.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2506.03135">OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models</a></td>
    <td>ARXIV</td>
    <td>25-06</td>
    <td><a href="https://huggingface.co/datasets/qizekun/OmniSpatial/tree/main">link</a></td>
    <td>0</td>
    <td>Image</td>
    <td><img src="excel_images/g8NiY-qr.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2505.17012">SpatialScore Towards Unified Evaluation for Multimodal Spatial Understanding</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td><a href="https://huggingface.co/datasets/haoningwu/SpatialScore">link</a></td>
    <td>0</td>
    <td>Image</td>
    <td><img src="excel_images/lhn2_c3x.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2505.19015">Can Multimodal Large Language Models Understand Spatial Relations</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td><a href="https://huggingface.co/datasets/liuziyan/SpatialMQA">link</a></td>
    <td>--</td>
    <td>Image</td>
    <td><img src="excel_images/fxTYRNlg.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2505.11907">Are Multimodal Large Language Models Ready for Omnidirectional Spatial Reasoning?</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td><a href="https://huggingface.co/datasets/UUUserna/OSR-Bench">link</a></td>
    <td>0</td>
    <td>Image</td>
    <td><img src="excel_images/Hlr9kzXY.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2505.05456">SITE: towards Spatial Intelligence Thorough Evaluation</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td><a href="https://huggingface.co/datasets/franky-veteran/SITE-Bench">link</a></td>
    <td>0</td>
    <td>Image/Multi-view Image/Video</td>
    <td><img src="excel_images/_LqO-k3V.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2505.20279">VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td><a href="https://huggingface.co/datasets/Journey9ni/vstibench">link</a></td>
    <td>0</td>
    <td>Video</td>
    <td><img src="excel_images/RuLIQBSm.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2505.23764">MMSI-Bench: A Benchmark for Multi-ImagecSpatial Intelligence</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td><a href="https://huggingface.co/datasets/RunsenXu/MMSI-Bench">link</a></td>
    <td>1</td>
    <td>multi-view</td>
    <td><img src="excel_images/OZKztnHG.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2505.21500">ViewSpatial-Bench:Evaluating Multi-perspective Spatial Localization in Vision-Language Models</a></td>
    <td>ARXIV</td>
    <td>25-05</td>
    <td><a href="https://huggingface.co/datasets/lidingm/ViewSpatial-Bench">link</a></td>
    <td>--</td>
    <td>multi-view</td>
    <td><img src="excel_images/eCim7Uwu.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2504.00883">Improved Visual-Spatial Reasoning via R1-Zero-Like Training</a></td>
    <td>ARXIV</td>
    <td>25-04</td>
    <td><a href="https://huggingface.co/datasets/OPPOer/VSI-100k">link</a></td>
    <td>9</td>
    <td>--</td>
    <td><img src="excel_images/opOZUSyP.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2504.15376">Towards Understanding Camera Motions in Any Video</a></td>
    <td>ARXIV</td>
    <td>25-04</td>
    <td><a href="https://huggingface.co/datasets/syCen/CameraBench">link</a></td>
    <td>1</td>
    <td>Video</td>
    <td><img src="excel_images/O83Du9Do.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2504.15280">Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs</a></td>
    <td>ARXIV</td>
    <td>25-04</td>
    <td><a href="https://huggingface.co/datasets/ch-chenyu/All-Angles-Bench">link</a></td>
    <td>3</td>
    <td>multi-view</td>
    <td><img src="excel_images/Cg5i_q-x.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2503.19707?">Mind the Gap: Benchmarking Spatial Reasoning in Vision-Language Models</a></td>
    <td>ARXIV</td>
    <td>25-03</td>
    <td><a href="https://huggingface.co/datasets/stogian/srbench">link</a></td>
    <td>5</td>
    <td>Image</td>
    <td><img src="excel_images/ADVkXqu3.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2503.11094">Open3DVQA: ABenchmark for Comprehensive Spatial Reasoning with Multimodal Large Language Model in Open Space</a></td>
    <td>OPEN3DVQA</td>
    <td>25-03</td>
    <td><a href="https://drive.google.com/drive/folders/1CKSavijr67U8jKMg_kpYNKKs9Nk_bmg1?usp=sharing">link</a></td>
    <td>4</td>
    <td>Image</td>
    <td><img src="excel_images/j2tWpOzV.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2503.19990">LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?</a></td>
    <td>ARXIV</td>
    <td>25-03</td>
    <td><a href="https://huggingface.co/datasets/KexianTang/LEGO-Puzzles">link</a></td>
    <td>8</td>
    <td>Multi-view</td>
    <td><img src="excel_images/mdUL2p0S.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2503.23765">STI-Bench: Are MLLMsReadyfor Precise Spatial-Temporal World Understanding?</a></td>
    <td>ARXIV</td>
    <td>25-03</td>
    <td>--</td>
    <td>7</td>
    <td>Video</td>
    <td><img src="excel_images/6aGr0Crg.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2503.12542?">ST-Think: How Multimodal Large Language Models Reason About 4D Worlds from Ego-Centric Videos</a></td>
    <td>ARXIV</td>
    <td>25-03</td>
    <td><a href="https://huggingface.co/datasets/openinterx/Ego-ST-bench">link</a></td>
    <td>5</td>
    <td>Video</td>
    <td><img src="excel_images/cmi2LbEh.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2503.22976?">From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D</a></td>
    <td>ARXIV</td>
    <td>25-03</td>
    <td><a href="https://huggingface.co/datasets/jasonzhango/SPAR-Bench">link</a></td>
    <td>2</td>
    <td>Image</td>
    <td><img src="excel_images/acv2sMW7.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2502.07183">Space-Aware Instruction Tuning: Dataset and Benchmark for Guide Dog Robots Assisting the Visually Impaired</a></td>
    <td>ARXIV</td>
    <td>25-02</td>
    <td><a href="https://docs.google.com/forms/d/e/1FAIpQLScBmoVoj0d-omBOVCHGjhRislXP0TYzRqaUJOmJcqN6ylQcxQ/viewform">link</a></td>
    <td>1</td>
    <td>Image</td>
    <td><img src="excel_images/PTkGmI15.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2502.06787">VADAR: Visual Agentic AI for Spatial Reasoning with a Dynamic API</a></td>
    <td>CVPR</td>
    <td>25-02</td>
    <td><a href="https://huggingface.co/datasets/dmarsili/Omni3D-Bench">link</a></td>
    <td>5</td>
    <td>image</td>
    <td><img src="excel_images/PTe2wL1E.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2501.16411">PHYSBENCH: BENCHMARKING AND ENHANCING VISION-LANGUAGE MODELS FOR PHYSICAL WORLD UNDERSTANDING</a></td>
    <td>ARXIV</td>
    <td>25-01</td>
    <td><a href="https://huggingface.co/datasets/USC-GVL/PhysBench">link</a></td>
    <td>22</td>
    <td>Image/Video</td>
    <td><img src="excel_images/rVLANkzs.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2501.09024">Social-LLaVA: Enhancing Robot Navigation through Human-Language Reasoning in Social Spaces</a></td>
    <td>ARXIV</td>
    <td>25-01</td>
    <td><a href="https://huggingface.co/datasets/amir-pyh/SNEI-v0-free-form-325">link</a></td>
    <td>2</td>
    <td>Image</td>
    <td><img src="excel_images/tYtgNWTo.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2501.15830">SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model</a></td>
    <td>ARXIV</td>
    <td>25-01</td>
    <td><a href="https://github.com/SpatialVLA/SpatialVLA?tab=readme-ov-file">link</a></td>
    <td>--</td>
    <td>Video</td>
    <td><img src="excel_images/M1u8eEkp.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2412.11974">Emma-X: An Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning</a></td>
    <td>ARXIV</td>
    <td>24-12</td>
    <td><a href="https://huggingface.co/declare-lab/Emma-X/tree/main">link</a></td>
    <td>--</td>
    <td>Video</td>
    <td><img src="excel_images/uK1ALtMG.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2412.06322">LLaVA-SpaceSGG: Visual Instruct Tuning for Open-vocabulary Scene Graph Generation with Enhanced Spatial Relations</a></td>
    <td>ARXIV</td>
    <td>24-12</td>
    <td><a href="https://huggingface.co/datasets/Endlinc/SpaceSGG-Val">link</a></td>
    <td>2</td>
    <td>image</td>
    <td><img src="excel_images/iF4ygXnb.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2412.07825">3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark</a></td>
    <td>ARXIV</td>
    <td>24-12</td>
    <td><a href="https://huggingface.co/datasets/ccvl/3DSRBench">link</a></td>
    <td>12</td>
    <td>Image</td>
    <td><img src="excel_images/HQn-vXEm.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2412.12693">SPHERE: Unveiling Spatial Blind Spots in Vision-Language Models Through Hierarchical Evaluation</a></td>
    <td>ACL</td>
    <td>24-12</td>
    <td><a href="https://huggingface.co/datasets/wei2912/SPHERE-VLM">link</a></td>
    <td>3</td>
    <td>Image</td>
    <td><img src="excel_images/0lhZyChh.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2412.14171">Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces</a></td>
    <td>ARXIV</td>
    <td>24-12</td>
    <td><a href="https://huggingface.co/datasets/nyu-visionx/VSI-Bench">link</a></td>
    <td>86</td>
    <td>Video</td>
    <td><img src="excel_images/fcQJhwEt.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2411.15714">ROOT: VLM-based System for Indoor Scene Understanding and Beyond</a></td>
    <td>ARXIV</td>
    <td>24-11</td>
    <td>--</td>
    <td>2</td>
    <td>image</td>
    <td><img src="excel_images/I-qtoWS6.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2411.06048">An Empirical Analysis on Spatial Reasoning Capabilities of
Large Multimodal Models</a></td>
    <td>EMNLP 2024</td>
    <td>24-11</td>
    <td><a href="https://github.com/FatemehShiri/Spatial-MM">link</a></td>
    <td>9</td>
    <td>Image</td>
    <td><img src="excel_images/eWmzupK8.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2411.19325">GEOBench-VLM: Benchmarking Vision-Language Models for Geospatial Tasks</a></td>
    <td>ARXIV</td>
    <td>24-11</td>
    <td><a href="https://github.com/The-AI-Alliance/GEO-Bench-VLM">link</a></td>
    <td>7</td>
    <td>Image</td>
    <td><img src="excel_images/XLUydYIU.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Majumdar_OpenEQA_Embodied_Question_Answering_in_the_Era_of_Foundation_Models_CVPR_2024_paper.pdf">OpenEQA: Embodied Question Answering in the Era of Foundation Models</a></td>
    <td>CVPR</td>
    <td>24-11</td>
    <td><a href="https://github.com/facebookresearch/open-eqa/tree/main/data">link</a></td>
    <td>149</td>
    <td>Video</td>
    <td><img src="excel_images/ima3AMa4.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2410.06468?">DOES SPATIAL COGNITION EMERGE IN FRONTIER MODELS?</a></td>
    <td>ICLR</td>
    <td>24-10</td>
    <td><a href="https://github.com/apple/ml-space-benchmark">link</a></td>
    <td>19</td>
    <td>Video</td>
    <td><img src="excel_images/eHMAtaJP.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2409.09788">Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models</a></td>
    <td>ARXIV</td>
    <td>24-09</td>
    <td><a href="https://huggingface.co/datasets/andrewliao11/Q-Spatial-Bench">link</a></td>
    <td>15</td>
    <td>Image</td>
    <td><img src="excel_images/BI-QJVqK.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2408.11748">Understanding Depth and Height Perception of Large Visual-Language Models</a></td>
    <td>CVPRW</td>
    <td>24-08</td>
    <td><a href="https://drive.google.com/drive/folders/13WqSuUNV007oQp-u4t0FA51ZOI7BqAaG">link</a></td>
    <td>0</td>
    <td>2D/3D image</td>
    <td><img src="excel_images/eCqv5XLg.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2407.01863">VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs</a></td>
    <td>ARXIV</td>
    <td>24-07</td>
    <td><a href="https://github.com/UCSB-NLP-Chang/Visual-Spatial-Planning">link</a></td>
    <td>4</td>
    <td>Image</td>
    <td><img src="excel_images/uvxI8qwE.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2406.04566">SpaRC and SpaRP: Spatial Reasoning Characterization and Path Generation for Understanding Spatial Reasoning Capability of Large Language Models</a></td>
    <td>ACL</td>
    <td>24-06</td>
    <td><a href="https://huggingface.co/datasets/UKPLab/sparp">link</a></td>
    <td>4</td>
    <td>text</td>
    <td><img src="excel_images/b854QbdP.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2406.13642">SpatialBot: Precise Spatial Understanding with Vision Language Models</a></td>
    <td>ICRA</td>
    <td>24-06</td>
    <td><a href="https://huggingface.co/datasets/RussRobin/SpatialQA">link</a></td>
    <td>43</td>
    <td>Image</td>
    <td><img src="excel_images/f4xGRR29.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2406.01584">SpatialRGPT: Grounded Spatial Reasoning in Vision-Language Models</a></td>
    <td>ARXIV</td>
    <td>24-06</td>
    <td><a href="https://huggingface.co/datasets/a8cheng/SpatialRGPT-Bench">link</a></td>
    <td>104</td>
    <td>Image, point cloud</td>
    <td><img src="excel_images/vjLeTN6q.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2406.14852">Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models</a></td>
    <td>NEURIPS</td>
    <td>24-06</td>
    <td><a href="https://huggingface.co/datasets/MilaWang/SpatialEval">link</a></td>
    <td>61</td>
    <td>text only/image only/image-text</td>
    <td><img src="excel_images/_AY7q3f3.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2406.05756">EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models</a></td>
    <td>ACL 2024 SHORT</td>
    <td>24-06</td>
    <td><a href="https://huggingface.co/datasets/Phineas476/EmbSpatial-Bench">link</a></td>
    <td>22</td>
    <td>Image</td>
    <td><img src="excel_images/DCOWsPKa.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2406.00622">COMPOSITIONAL 4D DYNAMIC SCENES UNDERSTANDING WITH PHYSICS PRIORS FOR VIDEO QUESTION ANSWERING</a></td>
    <td>ICLR2025</td>
    <td>24-06</td>
    <td><a href="https://huggingface.co/datasets/RyanWW/DynSuperCLEVR">link</a></td>
    <td>5</td>
    <td>Video</td>
    <td><img src="excel_images/YTPW-pAa.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2405.15064">Reframing Spatial Reasoning Evaluation in Language Models:
A Real-World Simulation Benchmark for Qualitative Reasoning</a></td>
    <td>IJCAI 2024</td>
    <td>24-05</td>
    <td><a href="https://huggingface.co/datasets/Fangjun/RoomSpace">link</a></td>
    <td>9</td>
    <td>Multi-view</td>
    <td><img src="excel_images/bp_2KaBY.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2404.03622">Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models</a></td>
    <td>NEURIPS</td>
    <td>24-04</td>
    <td><a href="https://github.com/microsoft/visualization-of-thought/raw/main/vot-dataset-visual-tasks.zip">link</a></td>
    <td>33</td>
    <td>image</td>
    <td><img src="excel_images/gcOnKwaw.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2404.12390">BLINK: Multimodal Large Language Models Can See but Not Perceive</a></td>
    <td>ECCV</td>
    <td>24-04</td>
    <td><a href="https://huggingface.co/datasets/BLINK-Benchmark/BLINK">link</a></td>
    <td>180</td>
    <td>Image/Multi-view Image</td>
    <td><img src="excel_images/Vuacqg9Q.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2403.16999">Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning</a></td>
    <td>NEURIPS</td>
    <td>24-03</td>
    <td><a href="https://huggingface.co/datasets/deepcs233/Visual-CoT">link</a></td>
    <td>74</td>
    <td>image</td>
    <td><img src="excel_images/gqbsrNRF.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2403.00729">Can Transformers Capture Spatial Relations between Objects?</a></td>
    <td>ICLR</td>
    <td>24-03</td>
    <td><a href="https://github.com/AlvinWen428/spatial-relation-benchmark">link</a></td>
    <td>6</td>
    <td>Image</td>
    <td><img src="excel_images/gqvUhRUc.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2401.00988">Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models</a></td>
    <td>CVPR</td>
    <td>24-01</td>
    <td><a href="https://drive.google.com/file/d/1ybwvgnPFRVwmEAJudWwnGQ8wLitzQ82O/view">link</a></td>
    <td>--</td>
    <td>Video</td>
    <td><img src="excel_images/eP_b855Z.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2401.17862">Proximity QA: Unleashing the Power of Multi-Modal Large Language Models for Spatial Proximity Analysis</a></td>
    <td>ARXIV</td>
    <td>24-01</td>
    <td><a href="https://huggingface.co/Electronics/ProximityQA/tree/main„ÄÅhttps://github.com/NorthSummer/ProximityQA">link</a></td>
    <td>2</td>
    <td>Image</td>
    <td><img src="excel_images/d_0u6y88.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2310.17914">3D-Aware Visual Question Answering about Parts, Poses and Occlusions</a></td>
    <td>NIPS</td>
    <td>23-10</td>
    <td><a href="https://github.com/XingruiWang/superclevr-3D-question">link</a></td>
    <td>14</td>
    <td>Image</td>
    <td><img src="excel_images/9R7HwGe6.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2210.07474">Sqa3d: Situated question answering in 3d scenes</a></td>
    <td>ICLR</td>
    <td>22-10</td>
    <td><a href="https://zenodo.org/records/7792397#.ZCkprfFBx3g">link</a></td>
    <td>161</td>
    <td>point cloud</td>
    <td><img src="excel_images/0y2byLoS.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2112.10482">ScanQA: 3D Question Answering for Spatial Scene Understanding</a></td>
    <td>CVPR</td>
    <td>21-12</td>
    <td><a href="https://github.com/ATR-DBI/ScanQA/blob/main/docs/dataset.md">link</a></td>
    <td>234</td>
    <td>point cloud</td>
    <td><img src="excel_images/LnLntLiF.png" alt="img" /></td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/pdf/2003.14034">SPARE3D: A Dataset for SPAtial REasoning on Three-View Line Drawings</a></td>
    <td>CVPR2020</td>
    <td>20-03</td>
    <td><a href="https://drive.google.com/drive/folders/1Mi2KZyKAlUOGYRQTDz3E5nhiXY5GhUB2">link</a></td>
    <td>23</td>
    <td>multi-view</td>
    <td><img src="excel_images/ezULEOrm.png" alt="img" /></td>
  </tr>
</tbody>
</table>


# Acknowledgements

- [Awesome-Spatial-Reasoning](https://github.com/yyyybq/Awesome-Spatial-Reasoning)



# üåç Visitor Statistics

<div align="center">
  <a href="https://clustrmaps.com/site/1bnq4" title="Visit tracker">
    <img src="https://clustrmaps.com/map_v2.png?d=ttWSFIONVkO6LyFMtAy5TPO-FY2eA7gpMuFq_zgR6_A&cl=ffffff" />
  </a>
</div>


